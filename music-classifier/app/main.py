# main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import joblib
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import uvicorn

# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
pipeline = None
sentence_model = None

# --- ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ decade_of_release Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ ---
def normalize_decade(value):
    # ĞµÑĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğµ â€” ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ´Ğ¾ Ğ´ĞµÑÑÑ‚Ğ¸Ğ»ĞµÑ‚Ğ¸Ñ
    try:
        val = int(value)
        if val >= 1960 and val < 1970:
            return '60'
        elif val >= 1970 and val < 1980:
            return '70'
        elif val >= 1980 and val < 1990:
            return '80'
        elif val >= 1990 and val < 2000:
            return '90'
        elif val >= 2000 and val < 2010:
            return '0'
        elif val >= 2010 and val < 2020:
            return '10'
        else:
            return 'unknown'
    except Exception:
        # ĞµÑĞ»Ğ¸ ÑƒĞ¶Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ°, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ
        return str(value)

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
@asynccontextmanager
async def lifespan(app):
    global pipeline, sentence_model

    try:
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ pipeline
        pipeline = joblib.load('xgb_pipe.joblib')
        print("âœ… Pipeline successfully loaded")

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°
        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Sentence Transformer model loaded")

    except Exception as e:
        print(f"âŒ Error loading models: {e}")
        raise RuntimeError("Model loading failed") from e

    yield  # ĞŸÑ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾

    # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸
    print("ğŸ”„ Shutting down application...")


# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
app = FastAPI(
    title="Music Track Classifier API",
    description="API Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞºĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ BERT-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ ML-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
    version="1.0",
    lifespan=lifespan
)


# ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
class TrackRequest(BaseModel):
    artist: str
    track: str
    decade_of_release: int
    danceability: float
    energy: float
    key: int
    loudness: float
    mode: int
    speechiness: float
    acousticness: float
    instrumentalness: float
    liveness: float
    valence: float
    tempo: float
    duration_ms: int
    time_signature: int
    chorus_hit: float
    sections: int


class BatchRequest(BaseModel):
    items: List[TrackRequest]


@app.post("/predict")
async def predict(request: TrackRequest):
    try:
        input_df = pd.DataFrame([request.dict()])
        input_df['decade_of_release'] = input_df['decade_of_release'].apply(normalize_decade)
        # --- Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ---
        track_embedding = sentence_model.encode(
            input_df['track'].values,
            show_progress_bar=False
        )
        for i in range(track_embedding.shape[1]):
            input_df[f'track_emb_{i}'] = track_embedding[:, i]

        input_df = input_df.drop(columns=['track'])

        # --- ĞĞ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº ---
        if 'artist' not in input_df.columns:
            input_df['artist'] = 'unknown_artist'
        if 'decade_of_release' not in input_df.columns:
            input_df['decade_of_release'] = 'unknown_decade'

        # --- ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ñ‚Ğ¸Ğ¿Ñ‹ ---
        input_df['artist'] = input_df['artist'].astype(str)
        input_df['decade_of_release'] = input_df['decade_of_release'].astype(str)

        # Ğ’ÑĞµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ â€” Ñ‡Ğ¸ÑĞ»Ğ°
        for col in input_df.columns:
            if col not in ['artist', 'decade_of_release']:
                input_df[col] = pd.to_numeric(input_df[col], errors='coerce')

        input_df = input_df.fillna(0.0)

        # --- ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ---
        prediction = pipeline.predict(input_df)
        probabilities = pipeline.predict_proba(input_df)

        return {
            "prediction": int(prediction[0]),
            "probabilities": probabilities[0].tolist(),
            "track_embedding_dim": int(track_embedding.shape[1]),
            "input_shape": input_df.shape   
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction error: {e}")



# --- Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ---
@app.post("/batch_predict")
async def batch_predict(request: BatchRequest):
    try:
        input_data = [item.dict() for item in request.items]
        input_df = pd.DataFrame(input_data)
        input_df['decade_of_release'] = input_df['decade_of_release'].apply(normalize_decade)

        # --- Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ---
        track_embeddings = sentence_model.encode(
            input_df['track'].values,
            show_progress_bar=False,
            batch_size=32
        )
        for i in range(track_embeddings.shape[1]):
            input_df[f'track_emb_{i}'] = track_embeddings[:, i]

        input_df = input_df.drop(columns=['track'])

        # --- Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº ---
        if 'artist' not in input_df.columns:
            input_df['artist'] = 'unknown_artist'
        if 'decade_of_release' not in input_df.columns:
            input_df['decade_of_release'] = 'unknown_decade'

        # --- Ğ¢Ğ¸Ğ¿Ñ‹ ---
        input_df['artist'] = input_df['artist'].astype(str)
        input_df['decade_of_release'] = input_df['decade_of_release'].astype(str)
        for col in input_df.columns:
            if col not in ['artist', 'decade_of_release']:
                input_df[col] = pd.to_numeric(input_df[col], errors='coerce')

        input_df = input_df.fillna(0.0)

        # --- ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ---
        predictions = pipeline.predict(input_df)
        probabilities = pipeline.predict_proba(input_df)

        results = [
            {
                "track": request.items[i].track,
                "prediction": int(predictions[i]),
                "probabilities": probabilities[i].tolist()
            }
            for i in range(len(predictions))
        ]

        return {"results": results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Batch prediction error: {e}")


# Health check ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚
@app.get("/health")
async def health_check():
    return {
        "status": "OK",
        "model_loaded": hasattr(pipeline, "predict"),
        "embedding_model_loaded": hasattr(sentence_model, "encode")
    }


# Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞµÑ€Ğ²ĞµÑ€Ğ°
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
        timeout_keep_alive=120
    )