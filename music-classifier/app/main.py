# main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import joblib
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import uvicorn

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
pipeline = None
sentence_model = None

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
@asynccontextmanager
async def lifespan():
    global pipeline, sentence_model

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline
        pipeline = joblib.load('xgb_pipe.joblib')
        print("‚úÖ Pipeline successfully loaded")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞
        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("‚úÖ Sentence Transformer model loaded")

    except Exception as e:
        print(f"‚ùå Error loading models: {e}")
        raise RuntimeError("Model loading failed") from e

    yield  # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ

    # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    print("üîÑ Shutting down application...")


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Music Track Classifier API",
    description="API –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BERT-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ ML-–º–æ–¥–µ–ª–∏",
    version="1.0",
    lifespan=lifespan
)


# –ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
class TrackRequest(BaseModel):
    artist: str
    track: str
    decade_of_release: int
    danceability: float
    energy: float
    key: int
    loudness: float
    mode: int
    speechiness: float
    acousticness: float
    instrumentalness: float
    liveness: float
    valence: float
    tempo: float
    duration_ms: int
    time_signature: int
    chorus_hit: float
    sections: int


class BatchRequest(BaseModel):
    items: List[TrackRequest]


# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
@app.post("/predict")
async def predict(request: TrackRequest):
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ DataFrame
        input_df = pd.DataFrame([request.dict()])

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç—Ä–µ–∫–∞
        track_embedding = sentence_model.encode(
            input_df['track'].values,
            show_progress_bar=False
        )

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –¥–∞–Ω–Ω—ã–µ
        for i in range(track_embedding.shape[1]):
            input_df[f'track_embed_{i}'] = track_embedding[:, i]

        # –£–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è
        input_df = input_df.drop(columns=['track'])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = pipeline.predict(input_df)
        probabilities = pipeline.predict_proba(input_df)

        return {
            "prediction": int(prediction[0]),
            "probabilities": probabilities[0].tolist(),
            "track_embedding_dim": track_embedding.shape[1]
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Prediction error: {str(e)}"
        )


# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
@app.post("/batch_predict")
async def batch_predict(request: BatchRequest):
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ DataFrame
        input_data = [item.dict() for item in request.items]
        input_df = pd.DataFrame(input_data)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ç—Ä–µ–∫–æ–≤
        track_embeddings = sentence_model.encode(
            input_df['track'].values,
            show_progress_bar=False,
            batch_size=32
        )

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –¥–∞–Ω–Ω—ã–µ
        for i in range(track_embeddings.shape[1]):
            input_df[f'track_embed_{i}'] = track_embeddings[:, i]

        # –£–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è
        input_df = input_df.drop(columns=['track'])

        # –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = pipeline.predict(input_df)
        probabilities = pipeline.predict_proba(input_df)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        results = []
        for i in range(len(predictions)):
            results.append({
                "prediction": int(predictions[i]),
                "probabilities": probabilities[i].tolist(),
                "track": request.items[i].track  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            })

        return {"results": results}

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Batch prediction error: {str(e)}"
        )


# Health check —ç–Ω–¥–ø–æ–∏–Ω—Ç
@app.get("/health")
async def health_check():
    return {
        "status": "OK",
        "model_loaded": hasattr(pipeline, "predict"),
        "embedding_model_loaded": hasattr(sentence_model, "encode")
    }


# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
        timeout_keep_alive=120
    )