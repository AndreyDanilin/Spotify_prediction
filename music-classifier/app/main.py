# main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import joblib
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import uvicorn

# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
pipeline = None
sentence_model = None

# --- ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ decade_of_release Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ ---
def normalize_decade(value):
    # ĞµÑĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğµ â€” ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ´Ğ¾ Ğ´ĞµÑÑÑ‚Ğ¸Ğ»ĞµÑ‚Ğ¸Ñ
    try:
        val = int(value)
        if val >= 1960 and val < 1970:
            return '60'
        elif val >= 1970 and val < 1980:
            return '70'
        elif val >= 1980 and val < 1990:
            return '80'
        elif val >= 1990 and val < 2000:
            return '90'
        elif val >= 2000 and val < 2010:
            return '0'
        elif val >= 2010 and val < 2020:
            return '10'
        else:
            return 'unknown'
    except Exception:
        # ĞµÑĞ»Ğ¸ ÑƒĞ¶Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ°, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ
        return str(value)

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
@asynccontextmanager
async def lifespan(app):
    global pipeline, sentence_model

    try:
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ pipeline
        pipeline = joblib.load('xgb_pipe.joblib')
        print("âœ… Pipeline successfully loaded")

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°
        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Sentence Transformer model loaded")

    except Exception as e:
        print(f"âŒ Error loading models: {e}")
        raise RuntimeError("Model loading failed") from e

    yield  # ĞŸÑ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾

    # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸
    print("ğŸ”„ Shutting down application...")


# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
app = FastAPI(
    title="Music Track Classifier API",
    description="API Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞºĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ BERT-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ ML-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
    version="1.0",
    lifespan=lifespan
)


class TrackRequest(BaseModel):
    artist: str
    decade_of_release: str
    danceability: float
    energy: float
    key: int
    loudness: float
    mode: int
    speechiness: float
    acousticness: float
    instrumentalness: float
    liveness: float
    valence: float
    tempo: float
    duration_ms: int
    time_signature: int
    chorus_hit: float
    sections: int
    track_emb: List[float]

class BatchRequest(BaseModel):
    items: List[TrackRequest]


class BatchRequest(BaseModel):
    items: List[TrackRequest]


@app.post("/predict")
async def predict(request: TrackRequest):
    try:
        # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² DataFrame
        input_data = request.dict()
        # Ğ•ÑĞ»Ğ¸ track_emb ÑƒĞ¿Ğ°ĞºĞ¾Ğ²Ğ°Ğ½ Ğ² ÑĞ¿Ğ¸ÑĞ¾Ğº, Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ĞµĞ¼ ĞµĞ³Ğ¾
        if isinstance(input_data.get('track_emb'), list):
            for i, v in enumerate(input_data['track_emb']):
                input_data[f'track_emb_{i}'] = v
            del input_data['track_emb']
        input_df = pd.DataFrame([input_data])

        # --- Ğ•Ğ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: Ğ²Ñ‹Ğ·Ğ¾Ğ² pipeline ---
        prediction = pipeline.predict(input_df)
        probabilities = pipeline.predict_proba(input_df)

        return {
            "prediction": int(prediction[0]),
            "probabilities": probabilities[0].tolist(),
            "input_shape": input_df.shape
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction error: {e}")


# --- Ğ­Ğ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ---
@app.post("/batch_predict")
async def batch_predict(request: BatchRequest):
    try:
        input_data = []
        for item in request.items:
            d = item.dict()
            # Ğ•ÑĞ»Ğ¸ track_emb â€” ÑĞ¿Ğ¸ÑĞ¾Ğº, Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ĞµĞ¼ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹
            if isinstance(d.get('track_emb'), list):
                for i, v in enumerate(d['track_emb']):
                    d[f'track_emb_{i}'] = v
                del d['track_emb']
            input_data.append(d)
        input_df = pd.DataFrame(input_data)
        predictions = pipeline.predict(input_df)
        probabilities = pipeline.predict_proba(input_df)

        results = [
            {
                "prediction": int(predictions[i]),
                "probabilities": probabilities[i].tolist()
            }
            for i in range(len(predictions))
        ]
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Batch prediction error: {e}")


# Health check ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚
@app.get("/health")
async def health_check():
    return {
        "status": "OK",
        "model_loaded": hasattr(pipeline, "predict"),
        "embedding_model_loaded": hasattr(sentence_model, "encode")
    }


# Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞµÑ€Ğ²ĞµÑ€Ğ°
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
        timeout_keep_alive=120
    )